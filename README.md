# 										   				 AIMaaSå¹³å°-åŠ©åŠ›æ‚¨å¿«é€Ÿè½åœ°ç§æœ‰åŒ–çš„AIèƒ½åŠ›ï¼Œä¸€é”®å¼€å¯æ™ºèƒ½æ–°æ—¶ä»£ï¼Œå…¨é¢æ‹¥æŠ±æœªæ¥ï¼ ğŸ¤–

   AIMaaSå¹³å°æ˜¯ç”±å…«æ–—AIæŠ€æœ¯å›¢é˜Ÿç ”å‘çš„"ä¸€ç«™å¼æ¨¡å‹æœåŠ¡ç®¡ç†å¹³å°"ï¼Œæ˜¯ä¸€æ¬¾é›†æ¨¡å‹éƒ¨ç½²ä¸ç®¡ç†ã€æ¨¡å‹å¾®è°ƒè®­ç»ƒã€ç®—åŠ›èµ„æºç®¡ç†ç­‰åŠŸèƒ½äºä¸€ä½“çš„æ¨¡å‹æœåŠ¡å¹³å°ï¼Œç›®æ ‡æ˜¯å¸®åŠ©ä¼ä¸šå¿«é€Ÿå®ç°å¤§æ¨¡å‹ç§æœ‰åŒ–éƒ¨ç½²ä¸åº”ç”¨ï¼Œæ”¯æŒå°†ä¸€äº›ä¼ä¸šæˆ–è€…ç»„ç»‡ä¸“ä¸šçš„é—®ç­”åªæ˜¯å¾®è°ƒè‡³åº”ç”¨æ¨¡å‹ä¸­ï¼Œå¿«é€Ÿæ¨è¿›ä¼ä¸šä¸šåŠ¡æ™ºèƒ½åŒ–çš„å‘å±•ã€‚
   
ç‰¹ç‚¹åŒ…å«ï¼š
 - ä¸€é”®éƒ¨ç½²æ¨¡å‹ï¼šä¸€é”®éƒ¨ç½²Qwenã€Deepseekã€Glmã€LLamaç­‰å¤§è¯­è¨€æ¨¡å‹ã€‚å¯ä»¥çµæ´»è®¾å®šæ¨¡å‹ä½¿ç”¨å¤šå°‘å¼ æ˜¾å¡è¿›è¡Œå¯åŠ¨
 - ä¸€é”®å¯åœæ¨¡å‹ï¼šä¸€é”®å¯åŠ¨ä¸åœæ­¢æ¨¡å‹ã€‚
 - åœ¨çº¿ç›‘æ§æ¨¡å‹è¿è¡ŒçŠ¶æ€ï¼šåœ¨çº¿æŸ¥çœ‹æ¨¡å‹è¿è¡Œæ—¥å¿—ï¼Œäº†è§£æ¨¡å‹è¿è¡Œæƒ…å†µã€‚
 - ä¸€é”®å¼æ¨¡å‹å¾®è°ƒï¼šå¯ä»¥å¿«é€Ÿè¿›è¡Œæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒï¼Œç›®å‰æ”¯æŒå…¨å‚å¾®è°ƒã€qloraå¾®è°ƒã€loraå¾®è°ƒç­‰ã€‚
 - å¤šç®—åŠ›è®¾å¤‡ç»Ÿä¸€ç®¡ç†ï¼šæ”¯æŒç®¡ç†å¤šå°ç®—åŠ›æœåŠ¡å™¨èµ„æºï¼Œä¾¿äºè‡ªç”±æ‹“å±•ç®¡ç†ä¼ä¸šçš„ç®—åŠ›ã€‚

å¦‚æœ‰é—®é¢˜æˆ–è€…å•†ä¸šåˆä½œï¼Œå¯ä»¥ç›´æ¥ç»™æˆ‘ä»¬emailç•™è¨€ï¼šaiservice@badousoft.com

# æ¡ˆä¾‹ä»‹ç»
  ä¼ä¸šç§æœ‰åŒ–æ¨¡å‹éƒ¨ç½²åº”ç”¨æ¡ˆä¾‹ï¼ŒæŸä¼ä¸šé€šè¿‡AIMaaSå¹³å°ï¼Œå®ç°å¯¹3å°6å¡æœåŠ¡å™¨è¿›è¡Œéƒ¨ç½²ï¼Œé‡‡ç”¨å…¥é—¨çº§æ˜¾å¡2080TIè¿›è¡Œéƒ¨ç½²ã€‚

  æœåŠ¡å™¨é…ç½®ï¼š2Uã€512Gã€1Tç£ç›˜ é…ç½®ã€‚åƒå…†è·¯ç”±å™¨å®ç°æœåŠ¡å™¨äº’è”ã€‚

  - æœåŠ¡å™¨Aï¼šéƒ¨ç½²AIMaaSå¹³å°ï¼Œå®ç°å¯¹æ‰€æœ‰çš„ç®—åŠ›è®¾å¤‡è¿›è¡Œç®¡ç†ã€‚
  4å¡ï¼šè¿è¡ŒDeepSeek-r1-32bï¼Œæä¾›è¯­è¨€æ¨ç†æœåŠ¡ã€‚  ã€æ³¨ï¼š4å¡2080TIå»ºè®®ç”¨æˆ·æ•°è½½10ä»¥å†…ã€‚å¯ä»¥å‡çº§åˆ°4090æ˜¾å¡ï¼Œæå‡å¹¶å‘ä¸ä¼šè¯å“åº”é€Ÿåº¦ã€‘
  2å¡ï¼šè¿è¡ŒQwen2.5-vl-7bï¼Œæä¾›å›¾åƒè¯†åˆ«æ¨ç†æœåŠ¡

  - æœåŠ¡å™¨Bï¼šï¼ˆAIMaaSå¹³å°èŠ‚ç‚¹æœåŠ¡ï¼Œåªéœ€å…ƒå®‰è£…k8såŠ å…¥AIMaaSå¹³å°ç®¡ç†å³å¯ï¼‰ã€‚
  4å¡ï¼šè¿è¡ŒQwen3-32bï¼Œæä¾›è¯­è¨€äº¤äº’æœåŠ¡
  1å¡ï¼šè¿è¡ŒBge-large*ï¼Œå‘é‡åŒ–æ¨¡å‹
  1å¡ï¼šè¿è¡ŒBge-rerankæ¨¡å‹

  - æœåŠ¡å™¨Cï¼šï¼ˆAIMaaSå¹³å°èŠ‚ç‚¹æœåŠ¡ï¼Œåªéœ€å…ƒå®‰è£…k8såŠ å…¥AIMaaSå¹³å°ç®¡ç†å³å¯ï¼‰
  2å¡ï¼šè¿è¡ŒQwen2.5-14bï¼Œæä¾›è¯­è¨€æœåŠ¡ï¼Œç”¨äºé—®é¢˜åˆ†ç±»ï¼Œç®€å•é—®é¢˜å¤„ç†ï¼Œå·¥å…·è°ƒç”¨ç­‰
  4å¡ï¼šç”¨äºæ¨¡å‹å¾®è°ƒè®­ç»ƒï¼Œæ”¯æŒå¯¹7bã€14bä»¥ä¸‹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€æ³¨ï¼šæ ¹æ®ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œå¯èƒ½æ—¶é—´å‘¨æœŸæ¯”è¾ƒé•¿ï¼Œä½†å¦‚æœåªæ˜¯loraæ¨¡å¼ä¸”è½®æ•°è¾ƒå°‘ï¼Œå¯ä»¥è½»åº¦å¾®è°ƒæ¨¡å‹ï¼Œå¢åŠ å…³é”®ä¸šåŠ¡é—®é¢˜çš„è§£ç­”èƒ½åŠ›ä¸è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›ã€‘

  é€šè¿‡ä»¥ä¸Šé…ç½®å¯åŠ¨çš„æœåŠ¡ï¼Œå¯ä»¥æ¥å…¥Difyã€Fastgptã€RagFlowã€n8nè¿›è¡Œé›†æˆåº”ç”¨æœåŠ¡ã€‚
  å¦‚ï¼š
  å¯ä»¥é€šè¿‡å·¥ä½œæµå¼•ç”¨å¿«é€Ÿæ„å»ºæ™ºèƒ½å®¢æœã€æ™ºèƒ½åˆ›ä½œç­‰åº”ç”¨ã€‚

# å‰ç½®ç¯å¢ƒé…ç½®
   éƒ¨ç½²AIMaaSå¹³å°ï¼Œå»ºè®®ç”±ç‹¬ç«‹ç®—åŠ›çš„æœåŠ¡å™¨ï¼Œå»ºè®®é…ç½®ï¼š
- 1ã€ç»æµä½“éªŒç‰ˆæœ¬
   ä¸€å°æœåŠ¡å™¨2CPUã€12Gå†…å­˜ã€1Tç£ç›˜ï¼Œ1-Nå¼ å¡ã€‚å»ºè®®2-4å¼ æ˜¾å¡
- 2ã€ä¸“ä¸šè®­ç»ƒç‰ˆæœ¬
   2å°ä»¥ä¸Šçš„æœåŠ¡å™¨ï¼Œå•å°æœåŠ¡å™¨é…ç½®ï¼š2CPUã€512Gå†…å­˜ã€2Tç£ç›˜ï¼Œ4-8å¼ æ˜¾å¡ï¼Œæ¨è3090ã€4090ã€A100ã€L40ç­‰é…ç½®ä»¥ä¸Šçš„æ˜¾å¡ã€‚

å»ºè®®è¿è¡Œæ“ä½œç³»ç»Ÿï¼šcentos7.9+ï¼ŒUbuntu20+ä»¥ä¸Šã€‚


# æŠ€æœ¯åŸç†
   æœ¬é¡¹ç›®ä¸»è¦å³æˆäº†VLLMå’ŒLLamafactoryï¼Œæ— é¡»å‘½ä»¤è¡Œæ“ä½œï¼Œç›´æ¥åœ¨çº¿å¯è§†åŒ–ç®¡ç†æ¨¡å‹ã€‚æ‰€æœ‰çš„æ“ä½œéƒ½ä¼šè½¬æˆä»»åŠ¡è¿›è¡Œå‘½ä»¤çš„è°ƒåº¦ã€‚
   
# ğŸš¦ å¿«é€Ÿå¼€å§‹

```bash
docker run crpi-wfhl7cyuhi65rl7p.cn-guangzhou.personal.cr.aliyuncs.com/fadsii/badou-aimaas:1.0
```

æºç å¯åŠ¨

```python
git clone 
# åˆå§‹åŒ–å’Œè¿è¡Œå‰ç«¯
cd badouai-maas
npm install --registry=https://registry.npm.taobao.org
# å¯åŠ¨æœåŠ¡
npm run dev
 
# åˆå§‹åŒ–å’Œè¿è¡Œåç«¯ å›åˆ°é¡¹ç›®æ ¹ç›®å½•
cd aimaas-server
mvn package 
# æŠŠå·¥ç¨‹æ”¾åˆ°8080ç«¯å£çš„tomcatçš„webapps
# è¿è¡ŒæˆåŠŸå è®¿é—®http://127.0.0.1:8000/badouai-maas/#/
```

------

# å¯æä¾›çš„æœåŠ¡
 - 1ã€ååŠ©ç§æœ‰åŒ–ç»„å»ºç®—åŠ›æœåŠ¡å™¨ï¼Œä¾‹å¦‚ï¼š2å°6å¡æœåŠ¡å™¨çš„ç®—åŠ›é›†ç¾¤ã€‚ä½æˆæœ¬å¿«é€Ÿè½åœ°ä¼ä¸šç§æœ‰åŒ–AIèƒ½åŠ›ã€‚
 - 2ã€å¯å®šåˆ¶AIMaaSå¹³å°ï¼Œèå…¥ä¼ä¸šæœåŠ¡ä¸­ã€‚
 - 3ã€ååŠ©å¾®è°ƒæ¨¡å‹ï¼Œæä¾›æ–¹æ¡ˆå»ºè®®ã€‚
 ï¼ˆå¦‚æœ‰é—®é¢˜æˆ–è€…å•†ä¸šåˆä½œï¼Œå¯ä»¥ç›´æ¥ç»™æˆ‘ä»¬emailç•™è¨€ï¼šaiservice@badousoft.comï¼‰
 



# æ”¯æŒè®­ç»ƒçš„æ¨¡å‹

| æ¨¡å‹                                                         | æ¨¡å‹å¤§å°                         |
| ------------------------------------------------------------ | -------------------------------- |
| [Baichuan 2](https://huggingface.co/baichuan-inc)            | 7B/13B                           |
| [ChatGLM3](https://huggingface.co/THUDM)                     | 6B                               |
| [Command R](https://huggingface.co/CohereForAI)              | 35B/104B                         |
| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)            | 560M/1.1B/1.7B/3B/7.1B/176B      |
| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)    | 7B/16B/67B/236B                  |
| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)         | 236B/671B                        |
| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)  | 1.5B/7B/8B/14B/32B/70B/671B      |
| [Falcon](https://huggingface.co/tiiuae)                      | 7B/11B/40B/180B                  |
| [Falcon-H1](https://huggingface.co/tiiuae)                   | 0.5B/1.5B/3B/7B/34B              |
| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)     | 2B/7B/9B/27B                     |
| [Gemma 3/Gemma 3n](https://huggingface.co/google)            | 1B/4B/6B/8B/12B/27B              |
| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/THUDM)      | 9B/32B                           |
| [GLM-4.1V](https://huggingface.co/THUDM)*                    | 9B                               |
| [GPT-2](https://huggingface.co/openai-community)             | 0.1B/0.4B/0.8B/1.5B              |
| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)        | 1B/2B/3B/8B                      |
| [Hunyuan](https://huggingface.co/tencent/)                   | 7B                               |
| [Index](https://huggingface.co/IndexTeam)                    | 1.9B                             |
| [InternLM 2-3](https://huggingface.co/internlm)              | 7B/8B/20B                        |
| [InternVL 2.5-3](https://huggingface.co/OpenGVLab)           | 1B/2B/8B/14B/38B/78B             |
| [Kimi-VL](https://huggingface.co/moonshotai)                 | 16B                              |
| [Llama](https://github.com/facebookresearch/llama)           | 7B/13B/33B/65B                   |
| [Llama 2](https://huggingface.co/meta-llama)                 | 7B/13B/70B                       |
| [Llama 3-3.3](https://huggingface.co/meta-llama)             | 1B/3B/8B/70B                     |
| [Llama 4](https://huggingface.co/meta-llama)                 | 109B/402B                        |
| [Llama 3.2 Vision](https://huggingface.co/meta-llama)        | 11B/90B                          |
| [LLaVA-1.5](https://huggingface.co/llava-hf)                 | 7B/13B                           |
| [LLaVA-NeXT](https://huggingface.co/llava-hf)                | 7B/8B/13B/34B/72B/110B           |
| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)          | 7B/34B                           |
| [MiMo](https://huggingface.co/XiaomiMiMo)                    | 7B                               |
| [MiniCPM](https://huggingface.co/openbmb)                    | 0.5B/1B/2B/4B/8B                 |
| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb) | 8B                               |
| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)   | 8B/12B                           |
| [Mistral/Mixtral](https://huggingface.co/mistralai)          | 7B/8x7B/8x22B                    |
| [Mistral Small](https://huggingface.co/mistralai)            | 24B                              |
| [OLMo](https://huggingface.co/allenai)                       | 1B/7B                            |
| [PaliGemma/PaliGemma2](https://huggingface.co/google)        | 3B/10B/28B                       |
| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)            | 1.3B/2.7B                        |
| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)            | 4B/14B                           |
| [Phi-3-small](https://huggingface.co/microsoft)              | 7B                               |
| [Phi-4](https://huggingface.co/microsoft)                    | 14B                              |
| [Pixtral](https://huggingface.co/mistralai)                  | 12B                              |
| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen) | 0.5B/1.5B/3B/7B/14B/32B/72B/110B |
| [Qwen3 (MoE)](https://huggingface.co/Qwen)                   | 0.6B/1.7B/4B/8B/14B/32B/235B     |
| [Qwen2-Audio](https://huggingface.co/Qwen)                   | 7B                               |
| [Qwen2.5-Omni](https://huggingface.co/Qwen)                  | 3B/7B                            |
| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)       | 2B/3B/7B/32B/72B                 |
| [Seed Coder](https://huggingface.co/ByteDance-Seed)          | 8B                               |
| [Skywork o1](https://huggingface.co/Skywork)                 | 8B                               |
| [StarCoder 2](https://huggingface.co/bigcode)                | 3B/7B/15B                        |
| [TeleChat2](https://huggingface.co/Tele-AI)                  | 3B/7B/35B/115B                   |
| [XVERSE](https://huggingface.co/xverse)                      | 7B/13B/65B                       |
| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)             | 1.5B/6B/9B/34B                   |
| [Yi-VL](https://huggingface.co/01-ai)                        | 6B/34B                           |
| [Yuan 2](https://huggingface.co/IEITYuan)                    | 2B/51B/102B                      |



# æ”¯æŒéƒ¨ç½²çš„æ¨¡å‹

| æ¨¡å‹                                                  | æ¨¡å‹å¤§å°                     | æ ¸å¿ƒæ¨¡æ€         |
| ----------------------------------------------------- | ---------------------------- | ---------------- |
| [Llama 3-3.3](https://huggingface.co/meta-llama)      | 1B/3B/8B/70B                 | æ–‡æœ¬æ¨¡æ€         |
| [Mixtral 8x7B](https://huggingface.co/mistralai)      | 8x7B                         | æ–‡æœ¬æ¨¡æ€         |
| [Qwen3 (MoE)](https://huggingface.co/Qwen)            | 0.6B/1.7B/4B/8B/14B/32B/235B | æ–‡æœ¬æ¨¡æ€         |
| [InternVL 2.5-3](https://huggingface.co/OpenGVLab)    | 1B/2B/8B/14B/38B/78B         | å¤šæ¨¡æ€ï¼ˆæ–‡å›¾ï¼‰   |
| [LLaVA-NeXT](https://huggingface.co/llava-hf)         | 7B/8B/13B/34B/72B/110B       | å¤šæ¨¡æ€ï¼ˆæ–‡å›¾ï¼‰   |
| [Qwen2.5-VL](https://huggingface.co/Qwen)             | 2B/3B/7B/32B/72B             | å¤šæ¨¡æ€ï¼ˆæ–‡å›¾ï¼‰   |
| [Yi-VL](https://huggingface.co/01-ai)                 | 6B/34B                       | å¤šæ¨¡æ€ï¼ˆæ–‡å›¾ï¼‰   |
| [StarCoder 2](https://huggingface.co/bigcode)         | 3B/7B/15B                    | æ–‡æœ¬æ¨¡æ€ï¼ˆä»£ç ï¼‰ |
| [TeleChat2](https://huggingface.co/Tele-AI)           | 3B/7B/35B/115B               | æ–‡æœ¬æ¨¡æ€         |
| [Llama 3.2 Vision](https://huggingface.co/meta-llama) | 11B/90B                      | å¤šæ¨¡æ€ï¼ˆæ–‡å›¾ï¼‰   |
| [Phi-3](https://huggingface.co/microsoft)             | 4B/14B                       | æ–‡æœ¬æ¨¡æ€         |
| [ChatGLM3](https://huggingface.co/THUDM)              | 6B                           | æ–‡æœ¬æ¨¡æ€         |

> æ³¨ï¼šä»¥ä¸Šä»…å±•ç¤ºéƒ¨åˆ†æ”¯æŒéƒ¨ç½²çš„æ¨¡å‹ï¼Œå®Œæ•´æ¨¡å‹åˆ—è¡¨å¯å‰å¾€ aimaas å¹³å°æŸ¥çœ‹ã€‚



# æ¼”ç¤º

## æ¼”ç¤ºï¼šæ¨¡å‹ä¸€é”®éƒ¨ç½²æµç¨‹

å±•ç¤ºä»ã€Œé€‰æ‹©æ¨¡å‹ã€åˆ°ã€Œå¯åŠ¨æœåŠ¡ã€çš„å…¨æµç¨‹ï¼ŒåŒ…å«ç‰ˆæœ¬é€‰æ‹©ã€èµ„æºé…ç½®ï¼ˆCPU/GPUï¼‰ã€éƒ¨ç½²ç¡®è®¤åŠå¯åŠ¨æˆåŠŸæç¤ºã€‚

![](assets/deploy-model.gif)

## æ¼”ç¤ºï¼šåœ¨çº¿æ¨¡å‹å¾®è°ƒæ“ä½œå’Œå¾®è°ƒåæ¨¡å‹æ•ˆæœå¯¹æ¯”

å±•ç¤ºå¦‚ä½•ä¸Šä¼ å¾®è°ƒæ•°æ®é›†ã€é…ç½®å¾®è°ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€è¿­ä»£æ¬¡æ•°ç­‰ï¼‰ã€å¯åŠ¨å¾®è°ƒä»»åŠ¡åŠæŸ¥çœ‹å®æ—¶è®­ç»ƒè¿›åº¦ã€‚





# å¼•ç”¨ 

```markdown
@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}


@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{2023xtuner,
    title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},
    author={XTuner Contributors},
    howpublished = {\url{https://github.com/InternLM/xtuner}},
    year={2023}
}
```



**AIMAAS é¡¹ç›®è®¸å¯è¯**



æœ¬é¡¹ç›®ï¼ˆå«æºä»£ç ã€æ–‡æ¡£ç­‰ï¼‰å—ã€ŠGNU é€šç”¨å…¬å…±è®¸å¯è¯ç¬¬ 3 ç‰ˆã€‹ï¼ˆGPLv3ï¼‰åŠä»¥ä¸‹è¡¥å……æ¡æ¬¾çº¦æŸï¼š

1. **æˆæƒèŒƒå›´**ï¼šä¸ªäººéå•†ä¸šä½¿ç”¨å¯å…è´¹è·å–ã€ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ï¼Œè¡ç”Ÿä½œå“éœ€é€‚ç”¨ç›¸åŒè®¸å¯è¯ï¼›å•†ä¸šä½¿ç”¨ï¼ˆå«é›†æˆåˆ°å•†ä¸šäº§å“ã€å•†ä¸šæœåŠ¡ç­‰ï¼‰é¡»è·é¡¹ç›®æ–¹ä¹¦é¢æˆæƒå¹¶æ”¯ä»˜è´¹ç”¨ã€‚
2. **ä¼ æ’­ä¸ä¿®æ”¹**ï¼šä¼ æ’­æ—¶éœ€ä¿ç•™åŸå§‹ç‰ˆæƒåŠè®¸å¯è¯ä¿¡æ¯ï¼›ä¿®æ”¹åçš„ä½œå“é¡»æ˜ç¡®æ ‡æ³¨ä¿®æ”¹å†…å®¹ï¼Œä¸”å—æœ¬è®¸å¯è¯çº¦æŸã€‚
3. **é™åˆ¶ä¸å…è´£**ï¼šç¦æ­¢ç”¨äºè¿æ³•æˆ–ä¾µæƒæ´»åŠ¨ï¼›é¡¹ç›®æ–¹ä¸æä¾›ä»»ä½•æ‹…ä¿ï¼Œä½¿ç”¨é£é™©ç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…ã€‚
4. **é€‚ç”¨æ³•å¾‹**ï¼šå—ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç®¡è¾–ï¼Œäº‰è®®å…ˆåå•†ï¼Œåå•†ä¸æˆç”±é¡¹ç›®æ–¹æ‰€åœ¨åœ°æ³•é™¢ç®¡è¾–ã€‚

è¡¥å……æ¡æ¬¾ä¸ GPLv3 å†²çªæ—¶ï¼Œä»¥ GPLv3 å¼ºåˆ¶æ€§è§„å®šä¸ºå‡†ã€‚é¡¹ç›®æ–¹ä¿ç•™è®¸å¯è¯è§£é‡ŠåŠä¿®æ”¹æƒï¼Œä¿®æ”¹åé€šè¿‡å®˜æ–¹æ¸ é“å‘å¸ƒã€‚
